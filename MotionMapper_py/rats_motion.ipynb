{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rats Motion Mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import modules\n",
    "wavelet : run the wavelet transform;\n",
    "\n",
    "run_tsne : run tSNE;\n",
    "\n",
    "density_map : apply a gaussian convolution to the scatter points in the behavioral space;\n",
    "\n",
    "embedding_z : re-embedding using tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wavelet import *\n",
    "from run_tsne import *\n",
    "from density_map import *\n",
    "from embedding_z import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import multiprocessing as mp\n",
    "from math import factorial\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import scipy as sp\n",
    "import matplotlib.gridspec as grd\n",
    "from scipy.stats import gaussian_kde\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data\n",
    "\n",
    "1. The data come from Bence Olveczky and Jesse Marshall's lab\n",
    "\n",
    "2. The raw data set contains the 3D positions of 20 markers on rats over a time period of 21 days at a sampling rate of 300 Hz, thus a 22,902,319 $\\times$ 60 matrix. \n",
    "\n",
    "3. The twenty markers are labeled as HeadF, HeadB, HeadL, SpineF, SpineM, SpineL, Offset1, Offset2, HipL, HipR, ElbowL, ArmL, ShoulderL, ShoulderR, ElbowR, ArmR, KneeR, KneeL, ShinL, ShinR. \n",
    "\n",
    "4. A short version data is used here (300,000 consecutive time points extracted from the raw data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"data/rat_data_short.txt\"\n",
    "data = np.loadtxt(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation of data\n",
    "Translating the position values into segment lengths and joint angles to filter out variance. Using\n",
    "\n",
    "$$ ||L|| =  [\\sum_{i = x,y,z} abs(i_{1}-i_{2})^2 ]^{1/2}$$\n",
    "$$ \\Theta = arccos(\\frac{\\vec{u}-\\vec{v}}{||\\vec{u}||\\cdot||\\vec{v}||})$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse data, find segment lengths and joint angles\n",
    "HeadF = data[:,0:3]\n",
    "HeadB = data[:,3:6]\n",
    "HeadL = data[:,6:9]\n",
    "SpineF = data[:,9:12]\n",
    "SpineM = data[:,12:15]\n",
    "SpineL = data[:,15:18]\n",
    "Offset1 = data[:,18:21]\n",
    "Offset2 = data[:,21:24]\n",
    "HipL = data[:,24:27]\n",
    "HipR = data[:,27:30]\n",
    "ElbowL = data[:,30:33]\n",
    "ArmL = data[:, 33:36]\n",
    "ShoulderL = data[:, 36:39]\n",
    "ShoulderR = data[:, 39:42]\n",
    "ElbowR = data[:, 42:45]\n",
    "ArmR = data[:,45:48]\n",
    "KneeR = data[:,48:51]\n",
    "KneeL = data[:,51:54]\n",
    "ShinL = data[:, 54:57]\n",
    "ShinR = data[:, 57:60]\n",
    "\n",
    "def joint_angle(x1 = np.array([]), x2 = np.array([]), x3 = np.array([])):\n",
    "# x1, x2, x3 are your first, second, third points\n",
    "    angle = np.zeros(x1.shape[0])\n",
    "    for i in range(x1.shape[0]):\n",
    "        angle[i] = np.arccos(np.dot((x1[i]-x2[i]), (x3[i]-x2[i])) / (np.linalg.norm(x1[i]-x2[i]) * np.linalg.norm(x3[i]-x2[i])))            \n",
    "    return angle\n",
    "\n",
    "def calculate_combinations(n, r):\n",
    "    return factorial(n) // factorial(r) // factorial(n-r)\n",
    "\n",
    "def parse_body_parts(body_part):\n",
    "# a body part contains all relevant tracking data for a certain body part\n",
    "# e.g. for Head, is HeadF, HeadB, HeadL and SpineF\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    Segs = np.zeros((calculate_combinations(len(body_part),2),np.shape(body_part[0])[0]))\n",
    "    Joint_angles = np.zeros((calculate_combinations(len(body_part),3),np.shape(body_part[0])[0])) \n",
    "    for ele in itertools.combinations(body_part,2):\n",
    "        Segs[count1] = np.linalg.norm(ele[0]-ele[1], axis = 1)\n",
    "        count1 += 1\n",
    "    for ele in itertools.combinations(body_part,3):\n",
    "        Joint_angles[count2] = joint_angle(ele[0], ele[1], ele[2])\n",
    "        count2 += 1\n",
    "    return Segs,Joint_angles\n",
    "\n",
    "Head_seg,Head_angle = parse_body_parts((HeadF,HeadB,HeadL,SpineF))\n",
    "print('Done HEAD segment lengths and joint angles')\n",
    "Body_seg,Body_angle = parse_body_parts((SpineF,SpineM,SpineL,Offset1,Offset2))\n",
    "print('Done BODY segment lengths and joint angles')\n",
    "LimbL_seg,LimbL_angle = parse_body_parts((ElbowL,ArmL,ShoulderL,SpineF))\n",
    "print('Done LIMBL segment lengths and joint angles')\n",
    "LimbR_seg,LimbR_angle = parse_body_parts((ElbowR,ArmR,ShoulderR,SpineF))\n",
    "print('Done LIMBR segment lengths and joint angles')\n",
    "LegL_seg,LegL_angle = parse_body_parts((HipL,KneeL,ShinL,SpineL))\n",
    "print('Done LEGL segment lengths and joint angles')\n",
    "LegR_seg,LegR_angle = parse_body_parts((HipR,KneeR,ShinR,SpineL))\n",
    "print('Done LEGR segment lengths and joint angles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots of segment lengths and joint angles\n",
    "\n",
    "<img src=\"./plots/joint_angles.png\" /><img src=\"./plots/seg_lengths.png\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all data in one data set\n",
    "All_seg_angle = np.zeros((Head_seg.shape[0]+Head_angle.shape[0]+\n",
    "                          Body_seg.shape[0]+Body_angle.shape[0]+LimbL_seg.shape[0]+LimbL_angle.shape[0]+\n",
    "                          LimbR_seg.shape[0]+LimbR_angle.shape[0]+LegL_seg.shape[0]+LegL_angle.shape[0]+\n",
    "                          LegR_seg.shape[0]+LegR_angle.shape[0], data.shape[0]))\n",
    "All_seg_angle[:Head_seg.shape[0]] = Head_seg\n",
    "temp = Head_seg.shape[0]\n",
    "All_seg_angle[temp:temp+Head_angle.shape[0]] = Head_angle\n",
    "temp += Head_angle.shape[0]\n",
    "All_seg_angle[temp:temp+Body_seg.shape[0]] = Body_seg\n",
    "temp += Body_seg.shape[0]\n",
    "All_seg_angle[temp:temp+Body_angle.shape[0]] = Body_angle\n",
    "temp += Body_angle.shape[0]\n",
    "All_seg_angle[temp:temp+LimbL_seg.shape[0]] = LimbL_seg\n",
    "temp += LimbL_seg.shape[0]\n",
    "All_seg_angle[temp:temp+LimbL_angle.shape[0]] = LimbL_angle\n",
    "temp += LimbL_angle.shape[0]\n",
    "All_seg_angle[temp:temp+LimbR_seg.shape[0]] = LimbR_seg\n",
    "temp += LimbR_seg.shape[0]\n",
    "All_seg_angle[temp:temp+LimbR_angle.shape[0]] = LimbR_angle\n",
    "temp += LimbR_angle.shape[0]\n",
    "All_seg_angle[temp:temp+LegL_seg.shape[0]] = LegL_seg\n",
    "temp += LegL_seg.shape[0]\n",
    "All_seg_angle[temp:temp+LegL_angle.shape[0]] = LegL_angle\n",
    "temp += LegL_angle.shape[0]\n",
    "All_seg_angle[temp:temp+LegR_seg.shape[0]] = LegR_seg\n",
    "temp += LegR_seg.shape[0]\n",
    "All_seg_angle[temp:temp+LegR_angle.shape[0]] = LegR_angle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The wavelet transform\n",
    "The morlet continuous wavelet transform is implemented to measure the power across a range of frequencies for each variable. Unlike Fourier spectrogram, where the time window has a fixed size, the wavelet transform strikes a balance between time domain and frequency domain, where we can see low frequency components at long time windows and high frequency resolution and vice versa for high frequency components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the wavelet transform:\n",
    "\n",
    "The resulted power spectrum $S(f,k;t)$ possess 75 dyadically spaced frequency channels (1 - 150Hz) for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Wavelet Transform\n",
    "numPeriods = 75 #set number of period\n",
    "omega0 = 5\n",
    "samplingFreq = 300.\n",
    "dt = 1/samplingFreq\n",
    "# set frequency \n",
    "minF = 1.  \n",
    "maxF = 150.\n",
    "minT = 1/maxF; maxT = 1/minF;\n",
    "Ts = []; f = []\n",
    "for i in range(numPeriods):\n",
    "    Ts.append(minT*2**(i*np.log(maxT/minT)/(np.log(2)*(numPeriods-1))))\n",
    "f = [1/i for i in Ts]\n",
    "f.reverse()\n",
    "\n",
    "numModes = All_seg_angle.shape[0]   #running wavelet transform on first 20 eigenmodes\n",
    "N = np.shape(All_seg_angle)[1]  #number of time points\n",
    "amp = np.zeros((N, numModes*numPeriods))  \n",
    "\n",
    "# multiprocess \n",
    "pool = mp.Pool(processes=18)\n",
    "results = pool.map(fastWavelet_morlet_convolution_parallel,\n",
    "                   itertools.izip(itertools.chain(All_seg_angle[:numModes]),\n",
    "                                  itertools.repeat(f), itertools.repeat(omega0), itertools.repeat(dt),itertools.count(1)))\n",
    "pool.close()\n",
    "pool.join()\n",
    "              \n",
    "for i, result in zip(range(numModes), results):\n",
    "    amp[:,(i)*numPeriods:(i+1)*numPeriods] = np.transpose(result)\n",
    "    \n",
    "where_are_NaNs = np.isnan(amp)\n",
    "where_are_Infs = np.isinf(amp)\n",
    "amp[where_are_NaNs] = 1e-12\n",
    "amp[where_are_Infs] = 1e-12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the spectrogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "time = [i * dt for i in range(N)]\n",
    "T, F = np.meshgrid(time,f)\n",
    "\n",
    "gs = grd.GridSpec(3, 2,  width_ratios=[20,1], wspace=0.1,hspace=0.6)\n",
    "\n",
    "# image plot\n",
    "ax1 = plt.subplot(gs[0])\n",
    "a = np.transpose(amp[:,(0)*numPeriods:numPeriods+(0)*numPeriods])\n",
    "im = ax1.pcolormesh(T,F,a,cmap = 'CMRmap_r', norm=matplotlib.colors.LogNorm(vmin=a.min(), vmax=a.max()))\n",
    "plt.title('wavelet transform of segment length HeadF-HeadB')\n",
    "plt.ylabel('frequency')\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "colorAx = plt.subplot(gs[1])\n",
    "cb = plt.colorbar(im, cax = colorAx)\n",
    "cb.set_label('RWU')\n",
    "\n",
    "ax2 = plt.subplot(gs[2])\n",
    "a = np.transpose(amp[:,(6)*numPeriods:numPeriods+(6)*numPeriods])\n",
    "im = ax2.pcolormesh(T,F,a,cmap = 'CMRmap_r', norm=matplotlib.colors.LogNorm(vmin=a.min(), vmax=a.max()))\n",
    "plt.title('wavelet transform of joint angle HeadF-HeadB-HeadL')\n",
    "plt.ylabel('frequency')\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "colorAx = plt.subplot(gs[3])\n",
    "cb = plt.colorbar(im, cax = colorAx)\n",
    "cb.set_label('RWU')\n",
    "\n",
    "ax2 = plt.subplot(gs[4])\n",
    "a = np.transpose(amp[:,(10)*numPeriods:numPeriods+(10)*numPeriods])\n",
    "im = ax2.pcolormesh(T,F,a,cmap = 'CMRmap_r', norm=matplotlib.colors.LogNorm(vmin=a.min(), vmax=a.max()))\n",
    "plt.title('wavelet transform of segment length SpineF-SpineM')\n",
    "plt.xlabel('time (s)')\n",
    "plt.ylabel('frequency')\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "colorAx = plt.subplot(gs[5])\n",
    "cb = plt.colorbar(im, cax = colorAx)\n",
    "cb.set_label('RWU')\n",
    "\n",
    "plt.savefig('./plots/wavelet.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots of spectrogram\n",
    "<img src=\"./plots/wavelet.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our implementation, the input $X$ consists of 30,000 randomly sampled feature vectors as training set, so a 30,000 $\\times$ 5250 matrix, $X =\\{x_{1},x_{2},...,x_{30,000}\\}$. In tSNE, the conserved invariants of high dimensional input is a joint probability distribution $P$ over all pairs of non-identical points, with entries\n",
    "\\begin{equation}\n",
    "    p_{ij} = \\frac{exp(-\\Delta^2_{ij}/\\sigma)}{\\sum_k\\sum_{l\\neq k} exp(-\\Delta^2_{kl}/\\sigma)},\n",
    "\\end{equation}\n",
    "where $\\Delta^2_{ij}$ is the pairwise distance in euclidean space.\n",
    "\n",
    "For the output 2D map, denoted by $Y = \\{y_1,y_2,...,y_{30,000}\\}$, we aim to model it in such a way that its joint probability distribution $q_{ij}$ is as close as possible to $p_{ij}$. A reasonable measurement of the similarity is the Kullback-Leibler divergence between P and Q:\n",
    "\\begin{equation}\n",
    "    C(Y) = KL(P||Q) =\\sum_i\\sum_{j\\neq i}p_{ij}log\\frac{p_{ij}}{q_{ij}}.\n",
    "\\end{equation}\n",
    "We use a gradient descent algorithm to minimize $C(Y)$ as it is almost always non-convex.\n",
    "\n",
    "Now we have to define the joint probabilities $q_{ij}$ in the low-dimensional map. The key property of tSNE is that in the low-dimensional map the similarity between two points is proportional to a Student-t distribution:\n",
    "\\begin{equation}\n",
    "    q_{ij} = \\frac{(1+||y_i-y_j||^2)^{-1}}{\\sum_k\\sum_{l\\neq k}(1+||y_k-y_l||^2)^{-1}}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = 30000 #set size of training set\n",
    "batch_index = np.random.randint(300000, size = training_set)\n",
    "#np.savetxt('./data/training_batch_index_head.txt', batch_index)\n",
    "motion_map = np.zeros((30000,2))\n",
    "\n",
    "training_batch = amp[batch_index,:]\n",
    "results = tsne(training_batch, 2,perplexity = 30)\n",
    "motion_map = results\n",
    "np.savetxt('./data/motion_map_head.txt', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter plot of the 2D behavioral space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "x = motion_map_head[:,0]\n",
    "y = motion_map_head[:,1]\n",
    "xy = np.vstack([x,y])\n",
    "z = gaussian_kde(xy)(xy)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(x,y, c=z, s=1, edgecolor='')\n",
    "plt.xlim([motion_map.min(),motion_map.max()])\n",
    "plt.ylim([motion_map.min(),motion_map.max()])\n",
    "plt.title('scatter plot of training set')\n",
    "ax.set_aspect('auto')\n",
    "plt.savefig('./plots/motion_map_scatter.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./plots/motion_map_scatter.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Density plot\n",
    "\n",
    "Apply a Gaussian convolution to each point in the scatter map to obtain the density plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_density_map(motion_map, beta = 50,resolution = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./plots/motion_map.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-embedding the remaining points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "\n",
    "For a single point $Z$, the transition probability with the training set $p_{j|z}$ and transition probability in the embedded space $q_{j|\\zeta}$ are given by\n",
    "\n",
    "\\begin{align}\n",
    "     p_{j|z} & = \\frac{exp(-\\Delta^2_{jz}/\\sigma_z)}{\\sum_{k\\in \\text{training set}} exp(-\\Delta^2_{zk}/\\sigma_z)} \\\\\n",
    "     q_{j|\\zeta} &= \\frac{(1+||y_\\zeta-y_j||^2)^{-1}}{\\sum_{k'\\in \\text{embedding space}}(1+||y_\\zeta-y_{k'}||^2)^{-1}}.\n",
    "\\end{align}\n",
    "\n",
    "Similarly, for each $z$, we seek the $\\zeta$ that minimizes the cost function\n",
    "\n",
    "\\begin{equation}\n",
    "    C(z) = KL(p_{x|z}||q_{y|\\zeta}) =\\sum_{x\\in \\text{training set}}p_{x|z}log\\frac{p_{x|z}}{q_{y(x)|\\zeta}}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-embedding the time points 0 - 20 s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_values = embedding_Z (amp[:6000], amp[batch_index,:], motion_map, 1000)\n",
    "np.savetxt('./data/re_embedding.txt', embedding_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the re-embedding values $z_1$,  $z_2$:\n",
    "Dynamics in the 2D behavioral space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10,3))\n",
    "\n",
    "plt.plot(np.linspace(0,20,6000),embedding_values[:,0], label = 'z_1', linewidth = 0.5)\n",
    "plt.plot(np.linspace(0,20,6000),embedding_values[:,1], label = 'z_2', linewidth = 0.5)\n",
    "plt.title('two dimension re-embedding values')\n",
    "plt.xlabel('time (s)')\n",
    "plt.legend()\n",
    "plt.savefig('./plots/re-embedding.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./plots/re-embedding.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The velocity profiles of the re-embedding values feature a distribution of two-component log-Gaussian mixture model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import lognorm\n",
    "\n",
    "two_peak_velocity = np.zeros(5999)\n",
    "\n",
    "for i in range(5999):\n",
    "    velocity = np.sqrt((embedding_values[i+1,0]-embedding_values[i,0])**2 + (embedding_values[i+1,1]-embedding_values[i,1])**2)\n",
    "    two_peak_velocity[i] = velocity\n",
    "# plot histogram in log space\n",
    "ax = plt.subplot(111)\n",
    "y,x,_ = ax.hist(two_peak_velocity, bins=np.logspace(-2,3,200),label = 'pdf')\n",
    "ax.set_xscale(\"log\")\n",
    "x=(x[1:]+x[:-1])/2\n",
    "\n",
    "def log_gauss(x,mu,sigma,A):\n",
    "    return A*np.exp(-(np.log(x)-mu)**2/2/sigma**2)\n",
    "\n",
    "def bimodal(x,mu1,sigma1,A1,mu2,sigma2,A2):\n",
    "    return log_gauss(x,mu1,sigma1,A1)+log_gauss(x,mu2,sigma2,A2)\n",
    "\n",
    "(mu1,sigma1,A1,mu2,sigma2,A2),pcov = sp.optimize.curve_fit(bimodal,x,y)\n",
    "pdf = bimodal(x,mu1,sigma1,A1,mu2,sigma2,A2)\n",
    "ax.plot(x, pdf, 'r', label ='fitted  bimodal log_Gaussian')\n",
    "plt.xlabel('velocity')\n",
    "plt.legend()\n",
    "plt.savefig('./plots/two_peak.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./plots/two_peak.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
